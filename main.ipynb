{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and save raw bibliographic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T09:54:09.062758Z",
     "start_time": "2018-01-22T09:54:05.521621Z"
    },
    "collapsed": true
   },
   "source": [
    "We want to keep the part of the code that downloads the raw data separate from the rest. The reason is that we won't run it that many times.\n",
    "There are two good reasons for this:\n",
    "1. This is the code that takes most time, mainly because we need to wait for the API to serve the data.\n",
    "2. We don't want to exceed our [weekly quota](https://dev.elsevier.com/api_key_settings.html) by performing repeated queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose the search query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put together a query string.\n",
    "Here we specify a search phrase and (optionally) date filters, sorting and the amount of results to return.\n",
    "Here you will find [Elsevier's instructions on how to write a query](https://dev.elsevier.com/tecdoc_federated_search.html).\n",
    "A query is the *question* you ask the database.\n",
    "The following example retrieves the occurrences of `Q Fever` from 2006 onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T17:10:04.014630Z",
     "start_time": "2018-02-15T17:10:04.011627Z"
    }
   },
   "outputs": [],
   "source": [
    "query = ['PUBYEAR > 2005 AND TITLE-ABS-KEY(Q Fever)']\n",
    "query.append(\"view=COMPLETE\")\n",
    "query.append(\"count=25\")  # Our subscription allows up to 25 results at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following initializes the client and the doc_search object, and then executes the search.\n",
    "You should request a private API key and create your own [configuration file as described here](https://github.com/ElsevierDev/elsapy/blob/master/CONFIG.md).\n",
    "You should keep the file `config.json` private (i.e. out of a shared Git repository)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the search and save the results to a local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only variable that needs to be modified here is `filename`.\n",
    "Note that if the file already exists the search results will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code downloads the results and saves it in 2 different formats:\n",
    "* `.json` (the original format, as received from the API)\n",
    "* `.xlsx` (an Excel file, convenient for quick examination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "\n",
    "with open(\"config.json\") as con_file:\n",
    "    config = json.load(con_file)    \n",
    "client = ElsClient(config['apikey'])\n",
    "doc_srch = ElsSearch('&'.join(query), 'scopus')\n",
    "doc_srch.execute(client, get_all=True)\n",
    "\n",
    "file = filename + \".json\"\n",
    "with open(\"output/{}\".format(file), 'w') as f:\n",
    "    json.dump(doc_srch.results, f)\n",
    "print('Done saving {0} results to file \"{1}\".'.format(len(doc_srch.results), file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we initialize the interactive plotting (will be needed later).\n",
    "In order to keep the rest of the code cleaner I will move all the imports up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T09:27:13.509590Z",
     "start_time": "2018-04-05T09:27:12.224890Z"
    }
   },
   "outputs": [],
   "source": [
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas.io.json\n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "from helperfuncs import extract_auth, extract_keywords, count_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['PUBYEAR < 2006 AND TITLE-ABS-KEY(Q Fever)']\n",
    "query.append(\"view=COMPLETE\")\n",
    "query.append(\"count=25\")  # Our subscription allows up to 25 results at a time\n",
    "client = ElsClient(config['apikey'])\n",
    "doc_srch = ElsSearch('&'.join(query), 'scopus')\n",
    "doc_srch.execute(client, get_all=True)\n",
    "filename = \"before2006.json\"\n",
    "with open(\"output/{}\".format(filename), 'w') as json_file:\n",
    "    json.dump(doc_srch.results, json_file)\n",
    "print('Done saving {0} results to file \"{1}\".'.format(len(doc_srch.results), filename))\n",
    "\n",
    "query = ['PUBYEAR > 2005 AND TITLE-ABS-KEY(Q Fever)']\n",
    "query.append(\"view=COMPLETE\")\n",
    "query.append(\"count=25\")  # Our subscription allows up to 25 results at a time\n",
    "client = ElsClient(config['apikey'])\n",
    "doc_srch = ElsSearch('&'.join(query), 'scopus')\n",
    "doc_srch.execute(client, get_all=True)\n",
    "filename = \"after2005.json\"\n",
    "with open(\"output/{}\".format(filename), 'w') as json_file:\n",
    "    json.dump(doc_srch.results, json_file)\n",
    "print('Done saving {0} results to file \"{1}\".'.format(len(doc_srch.results), filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T09:59:36.025215Z",
     "start_time": "2018-01-22T09:59:36.009622Z"
    }
   },
   "source": [
    "## Clean up the raw data and remove invalid records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will depend on the data you obtain.\n",
    "Our data set here requires very little cleaning, but it is useful as an example.\n",
    "First we retrieve the results from the file we just created and load them into a *DataFrame* structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T17:22:02.018064Z",
     "start_time": "2018-02-15T17:22:01.158467Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"my_results.json\"\n",
    "with open(\"output/{}\".format(filename), 'r') as results_file:\n",
    "    results = json.load(results_file)\n",
    "df_results = pandas.io.json.json_normalize(results)\n",
    "print('Loaded {0} results from file \"{1}\".'.format(len(results), filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some records are \"conference reviews\", not articles, and are therefore not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T17:22:02.030074Z",
     "start_time": "2018-02-15T17:22:02.019064Z"
    }
   },
   "outputs": [],
   "source": [
    "conference_review = df_results[df_results['subtype'] == \"cr\"].index\n",
    "df_results.drop(conference_review, inplace=True)\n",
    "print(\"Removed {0} 'Conference Review' records.\".format(len(conference_review)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T10:20:03.369891Z",
     "start_time": "2018-01-22T10:20:03.354262Z"
    }
   },
   "source": [
    "A couple of records have no author data or title. We need to remove those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T17:22:02.051087Z",
     "start_time": "2018-02-15T17:22:02.031082Z"
    }
   },
   "outputs": [],
   "source": [
    "no_author_data = df_results[df_results['author'].isnull()].index\n",
    "df_results.drop(no_author_data, inplace=True)\n",
    "print(\"Removed {0} records missing author data.\".format(len(no_author_data)))\n",
    "\n",
    "no_title = df_results[df_results['dc:title'].isnull()].index\n",
    "df_results.drop(no_title, inplace=True)\n",
    "print(\"Removed {0} records with no title:\".format(len(no_title)))\n",
    "number_of_papers = len(df_results.index)\n",
    "\n",
    "print(\"Proceeding with the remaining ({0}) records\".format(number_of_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the invalid records, it is convenient to create a new DataFrame containing only the fields (columns) we are interested in visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T17:22:02.079108Z",
     "start_time": "2018-02-15T17:22:02.052089Z"
    }
   },
   "outputs": [],
   "source": [
    "rec_date = pd.to_datetime(df_results['prism:coverDate'])\n",
    "rec_auth = df_results['author'].map(extract_auth)\n",
    "rec_kw = df_results['authkeywords'].map(extract_keywords)\n",
    "rec_title = df_results['dc:title']\n",
    "dict_of_series = {\"Author\": rec_auth,\n",
    "                  \"Date\": rec_date,\n",
    "                  \"Keywords\": rec_kw,\n",
    "                  \"Num_of_keywords\": rec_kw.map(count_items),\n",
    "                  \"Title\": rec_title,\n",
    "                  \"Year\": rec_date.map(lambda x: x.year)}\n",
    "df = pd.DataFrame(dict_of_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-22T10:38:48.841827Z",
     "start_time": "2018-01-22T10:38:48.810622Z"
    }
   },
   "source": [
    "## Generate the visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the following plots will use matplotlib.\n",
    "Using Seaborn is optional, but the plots look (subjectively) better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T17:22:02.084110Z",
     "start_time": "2018-02-15T17:22:02.080108Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 shows the amount of papers (and related keywors) per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T17:22:02.159162Z",
     "start_time": "2018-02-15T17:22:02.085115Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_per_year = df['Year'].value_counts().sort_index()\n",
    "keywords_per_year = df.groupby(\"Year\")[\"Num_of_keywords\"].sum()\n",
    "fig02 = plt.figure()\n",
    "ax02 = fig02.add_subplot(1, 1, 1)\n",
    "ax02.set_title(\"Figure 1: Total number of papers and keywords per year (1990-2016)\")\n",
    "ax02.plot(papers_per_year, 'b-')\n",
    "ax02.set_ylabel('Total number of papers per year', color='b')\n",
    "ax02.tick_params('y', colors='b')\n",
    "\n",
    "ax02b = ax02.twinx()\n",
    "ax02b.plot(keywords_per_year, 'r--')\n",
    "ax02b.set_ylabel('Total number of keywords per year', color='r')\n",
    "ax02b.tick_params('y', colors='r')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
